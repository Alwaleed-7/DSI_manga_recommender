{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "import html\n",
    "import math\n",
    "import shutil\n",
    "import copy\n",
    "import os.path\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import urllib.request\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and merging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Mangas.csv', encoding=\"utf8\")\n",
    "df2 = pd.read_csv('Author.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df2, on='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=[\"title\", \"description\"], keep=\"first\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop = True)\n",
    "df = df.drop(labels=['Unnamed: 0_x', 'Unnamed: 80', 'Unnamed: 0_y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count_chapters']=df['chapters']\n",
    "df=df.drop(labels='chapters',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count_chapters'].replace('?','6',inplace=True)\n",
    "df['count_chapters'].replace('Oneshot','1',inplace=True)\n",
    "df['count_chapters'].replace('５','5',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count_chapters'] = pd.to_numeric(df['count_chapters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores for every feature\n",
    "author_score = 1.5\n",
    "artist_score = 1.4\n",
    "demographic_score = 0.9\n",
    "theme_score = 0.8\n",
    "genre_score = 0.8\n",
    "format_score = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_manga = ['Award Winning',\n",
    " 'Long Strip',\n",
    " 'Oneshot',\n",
    " '4-Koma',\n",
    " 'Web Comic',\n",
    " 'Anthology',\n",
    " 'Adaptation',\n",
    " 'Full Color',\n",
    " 'Official Colored',\n",
    " 'Fan Colored']\n",
    "\n",
    "theme_manga = [\n",
    " 'Martial Arts',\n",
    " 'Supernatural',\n",
    " 'School Life',\n",
    " 'Post-Apocalyptic',\n",
    " 'Cooking',\n",
    " 'Video Games',\n",
    " 'Traditional Games',\n",
    " 'Music',\n",
    " 'Delinquents',\n",
    " 'Magic',\n",
    " 'Mafia',\n",
    " 'Office Workers',\n",
    " 'Military',\n",
    " 'Survival',\n",
    " 'Virtual Reality',\n",
    " 'Police',\n",
    " 'Ninja',\n",
    " 'Time Travel',\n",
    " 'Aliens',\n",
    " 'Demons',\n",
    " 'Animals',\n",
    " 'Samurai',\n",
    " 'Vampires',\n",
    " 'Monsters',\n",
    " 'Reincarnation',\n",
    " 'Monster Girls',\n",
    " 'Ghosts',\n",
    " 'Zombies',\n",
    " 'Villainess']\n",
    "\n",
    "genre_manga = ['Action',\n",
    " 'Comedy',\n",
    " 'Drama',\n",
    " 'Fantasy',\n",
    " 'Adventure',\n",
    " 'Romance',\n",
    " 'Psychological',\n",
    " 'Slice of Life',\n",
    " 'Sports',\n",
    " 'Horror',\n",
    " 'Mystery',\n",
    " 'Historical',\n",
    " 'Tragedy',\n",
    " 'Sci-Fi',\n",
    " 'Mecha',\n",
    " 'Medical',\n",
    " 'Thriller',\n",
    " 'Philosophical',\n",
    " 'Crime',\n",
    " 'Isekai']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model without TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(manga):\n",
    "    # the index of the input manga\n",
    "    manga_ind = df.index[df['title'] == manga].tolist()[0]\n",
    "    scores = []\n",
    "    for j in range(len(df)):\n",
    "        if j != manga_ind:\n",
    "            score = 0\n",
    "            if df['author'][manga_ind] == df['author'][j]:\n",
    "                score += author_score\n",
    "            if df['artist'][manga_ind] == df['artist'][j]:\n",
    "                score += artist_score\n",
    "            for k in format_manga:\n",
    "                if df[k][manga_ind] == df[k][j]:\n",
    "                    score += format_score\n",
    "            for m in theme_manga:\n",
    "                if df[m][manga_ind] == df[m][j]:\n",
    "                    score += theme_score\n",
    "            for t in genre_manga:\n",
    "                if df[t][manga_ind] ==df[t][j]:\n",
    "                    score += genre_score\n",
    "            score += df['rating'][j] - df['rating'][manga_ind]\n",
    "    \n",
    "            scores.append([score, df['title'][j]])\n",
    "    scores.sort()\n",
    "    scores.reverse()\n",
    "    return scores[0:10]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score('one piece')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(str_raw, removeStopWords=False):\n",
    "    # bbcodes that our description will have in it\n",
    "    # https://github.com/CarlosEsco/Neko/blob/master/app/src/main/java/eu/kanade/tachiyomi/source/online/utils/MdUtil.kt\n",
    "    descriptionLanguages = [\n",
    "        \"Russian / Русский\",\n",
    "        \"[u]Russian\",\n",
    "        \"[b][u]Russian\",\n",
    "        \"[RUS]\",\n",
    "        \"Russian / Русский\",\n",
    "        \"Russian/Русский:\",\n",
    "        \"Russia/Русское\",\n",
    "        \"Русский\",\n",
    "        \"RUS:\",\n",
    "        \"[b][u]German / Deutsch\",\n",
    "        \"German/Deutsch:\",\n",
    "        \"Espa&ntilde;ol / Spanish\",\n",
    "        \"Spanish / Espa&ntilde;ol\",\n",
    "        \"Spanish / Espa & ntilde; ol\",\n",
    "        \"Spanish / Espa&ntilde;ol\",\n",
    "        \"[b][u]Spanish\",\n",
    "        \"[Espa&ntilde;ol]:\",\n",
    "        \"[b] Spanish: [/ b]\",\n",
    "        \"정보\",\n",
    "        \"Spanish/Espa&ntilde;ol\",\n",
    "        \"Espa&ntilde;ol / Spanish\",\n",
    "        \"Italian / Italiano\",\n",
    "        \"Italian/Italiano\",\n",
    "        \"\\r\\n\\r\\nItalian\\r\\n\",\n",
    "        \"Pasta-Pizza-Mandolino/Italiano\",\n",
    "        \"Persian /فارسی\",\n",
    "        \"Farsi/Persian/\",\n",
    "        \"Polish / polski\",\n",
    "        \"Polish / Polski\",\n",
    "        \"Polish Summary / Polski Opis\",\n",
    "        \"Polski\",\n",
    "        \"Portuguese (BR) / Portugu&ecirc;s\",\n",
    "        \"Portuguese / Portugu&ecirc;s\",\n",
    "        \"Português / Portuguese\",\n",
    "        \"Portuguese / Portugu\",\n",
    "        \"Portuguese / Portugu&ecirc;s\",\n",
    "        \"Portugu&ecirc;s\",\n",
    "        \"Portuguese (BR) / Portugu & ecirc;\",\n",
    "        \"Portuguese (BR) / Portugu&ecirc;\",\n",
    "        \"[PTBR]\",\n",
    "        \"R&eacute;sume Fran&ccedil;ais\",\n",
    "        \"R&eacute;sum&eacute; Fran&ccedil;ais\",\n",
    "        \"[b][u]French\",\n",
    "        \"French / Fran&ccedil;ais\",\n",
    "        \"Fran&ccedil;ais\",\n",
    "        \"[hr]Fr:\",\n",
    "        \"French - Français:\",\n",
    "        \"Turkish / T&uuml;rk&ccedil;e\",\n",
    "        \"Turkish/T&uuml;rk&ccedil;e\",\n",
    "        \"T&uuml;rk&ccedil;e\",\n",
    "        \"[b][u]Chinese\",\n",
    "        \"Arabic / العربية\",\n",
    "        \"العربية\",\n",
    "        \"[hr]TH\",\n",
    "        \"[b][u]Vietnamese\",\n",
    "        \"[b]Links:\",\n",
    "        \"[b]Link[/b]\",\n",
    "        \"Links:\",\n",
    "        \"[b]External Links\"\n",
    "    ]\n",
    "    englishDescriptionTags = [\n",
    "        \"[b][u]English:\",\n",
    "        \"[b][u]English\",\n",
    "        \"[English]:\",\n",
    "        \"[B][ENG][/B]\"\n",
    "    ]\n",
    "    bbcodes = [\n",
    "        \"[list]\",\n",
    "        \"[/list]\",\n",
    "        \"[*]\",\n",
    "        \"[hr]\",\n",
    "        \"[u]\",\n",
    "        \"[/u]\",\n",
    "        \"[b]\",\n",
    "        \"[/b]\"\n",
    "    ]\n",
    "\n",
    "    # remove all non-english descriptions\n",
    "    # this assumes the english one is first\n",
    "    for tag in descriptionLanguages:\n",
    "        str_raw = str_raw.split(tag, 1)[0]\n",
    "\n",
    "    # now remove all english tags which are no longer needed\n",
    "    for tag in englishDescriptionTags:\n",
    "        str_raw = str_raw.replace(tag, \"\")\n",
    "\n",
    "    # convert all works to lower case\n",
    "    # also remove multiple white space and replace with single\n",
    "    str_raw = html.unescape(str_raw)\n",
    "    str_raw = str_raw.lower()\n",
    "    str_raw = \" \".join(str_raw.split())\n",
    "\n",
    "    # run a second time now, but with all lower case\n",
    "    # for tag in descriptionLanguages:\n",
    "    #     str_raw = str_raw.split(tag.lower(), 1)[-1]\n",
    "    # for tag in englishDescriptionTags:\n",
    "    #     str_raw = str_raw.replace(tag.lower(), \"\")\n",
    "\n",
    "    # next clean the string from any bbcodes\n",
    "    for tag in bbcodes:\n",
    "        str_raw = str_raw.replace(tag, \"\")\n",
    "    str_raw = re.sub('\\[.*?]', '', str_raw)\n",
    "\n",
    "    # remove source parentheses typical of anilist\n",
    "    # Eg: (source: solitarycross), (source: eat manga)\n",
    "    str_raw = re.sub(r'\\(source: [^)]*\\)', '', str_raw)\n",
    "\n",
    "    # remove any html codes\n",
    "    str_raw = re.sub(r'<[^>]*>', r' ', str_raw)\n",
    "\n",
    "    # remove emails and urls\n",
    "    str_raw = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', str_raw, flags=re.MULTILINE)\n",
    "    str_raw = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', ' ', str_raw, flags=re.MULTILINE)\n",
    "    str_raw = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ', str_raw, flags=re.MULTILINE)\n",
    "\n",
    "    # Replace apostrophes with standard lexicons\n",
    "    str_raw = str_raw.replace(\"isn't\", \"is not\")\n",
    "    str_raw = str_raw.replace(\"aren't\", \"are not\")\n",
    "    str_raw = str_raw.replace(\"ain't\", \"am not\")\n",
    "    str_raw = str_raw.replace(\"won't\", \"will not\")\n",
    "    str_raw = str_raw.replace(\"didn't\", \"did not\")\n",
    "    str_raw = str_raw.replace(\"shan't\", \"shall not\")\n",
    "    str_raw = str_raw.replace(\"haven't\", \"have not\")\n",
    "    str_raw = str_raw.replace(\"hadn't\", \"had not\")\n",
    "    str_raw = str_raw.replace(\"hasn't\", \"has not\")\n",
    "    str_raw = str_raw.replace(\"don't\", \"do not\")\n",
    "    str_raw = str_raw.replace(\"wasn't\", \"was not\")\n",
    "    str_raw = str_raw.replace(\"weren't\", \"were not\")\n",
    "    str_raw = str_raw.replace(\"doesn't\", \"does not\")\n",
    "    str_raw = str_raw.replace(\"'s\", \" is\")\n",
    "    str_raw = str_raw.replace(\"'re\", \" are\")\n",
    "    str_raw = str_raw.replace(\"'m\", \" am\")\n",
    "    str_raw = str_raw.replace(\"'d\", \" would\")\n",
    "    str_raw = str_raw.replace(\"'ll\", \" will\")\n",
    "\n",
    "    # now clean stop words which are not helpful\n",
    "    # we want to basically just collect a bunch of words\n",
    "    stops = ['the', 'a', 'an', 'and', 'but', 'if', 'or', 'because', 'as', 'what', 'which', 'this', 'that', 'these',\n",
    "             'those', 'then', 'just', 'so', 'than', 'such', 'both', 'through', 'about', 'for', 'is', 'of', 'while',\n",
    "             'during', 'to']\n",
    "\n",
    "    # Remove punctuation and stop words\n",
    "    if removeStopWords:\n",
    "        str_raw = ''.join([c for c in str_raw if c not in punctuation])\n",
    "        str_raw = \" \".join([w for w in str_raw.split() if w.lower() not in stops])\n",
    "\n",
    "    # Remove all symbols (clean to normal english)\n",
    "    # str_raw = re.sub(r'[^A-Za-z0-9\\s]', r' ', str_raw)\n",
    "    str_raw = re.sub(r'\\n', r' ', str_raw)\n",
    "    # str_raw = re.sub(r'[0-9]', r'', str_raw)\n",
    "    str_raw = \" \".join(str_raw.split())\n",
    "\n",
    "    # return the final cleaned string\n",
    "    return str_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ind = df.index[df['description'].isnull() == False].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_ind:\n",
    "    df['description'][i] = clean_string(df['description'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus_into_tfidf(corpus):\n",
    "    # build a TF/IDF matrix for each paper\n",
    "    # https://markhneedham.com/blog/2016/07/27/scitkit-learn-tfidf-and-cosine-similarity-for-computer-science-papers/\n",
    "    # tf = TfidfVectorizer(strip_accents='ascii', analyzer='word', ngram_range=(1, 1),\n",
    "    #                      min_df=0.10, max_df=1.0, stop_words='english', max_features=50000, sublinear_tf=False)\n",
    "    tf = TfidfVectorizer(strip_accents='unicode', analyzer='word')\n",
    "    x = tf.fit_transform(corpus)\n",
    "    # print(tf.get_feature_names())\n",
    "    # with open(\"./output/vocab.json\", \"w\") as fp:\n",
    "    #     json.dump(dict(zip(tf.get_feature_names(), x.toarray()[0])), fp, indent=2)\n",
    "    return x\n",
    "\n",
    "\n",
    "def find_similar_tfidf(tfidf_matrix, corpus_index):\n",
    "    # top similar papers based on cosine similarity\n",
    "    cosine_similarities = linear_kernel(tfidf_matrix[corpus_index:corpus_index + 1], tfidf_matrix).flatten()\n",
    "    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != corpus_index]\n",
    "\n",
    "    # return the matches (best matches to worst)\n",
    "    tuple_vec = [(index, cosine_similarities[index]) for index in related_docs_indices]\n",
    "\n",
    "    # convert to dictionary\n",
    "    scores = {}\n",
    "    for id1, score in tuple_vec:\n",
    "        scores[id1] = score\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset='description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_matrix = load_corpus_into_tfidf(df['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_tfidf_score_above_this_val = 0.30\n",
    "ignore_tfidf_score_below_this_val = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(manga):\n",
    "    manga_ind = df.index[df['title'] == manga].tolist()[0]\n",
    "    scores = []\n",
    "    tf_idf = find_similar_tfidf(my_matrix, manga_ind)\n",
    "    for j in range(len(df)):\n",
    "        if j != manga_ind:\n",
    "            score = 0\n",
    "            if df['author'][manga_ind] == df['author'][j]:\n",
    "                score += author_score\n",
    "            if df['artist'][manga_ind] == df['artist'][j]:\n",
    "                score += artist_score\n",
    "            for k in format_manga:\n",
    "                if df[k][manga_ind] == df[k][j]:\n",
    "                    score += format_score\n",
    "            for m in theme_manga:\n",
    "                if df[m][manga_ind] == df[m][j]:\n",
    "                    score += theme_score\n",
    "            for t in genre_manga:\n",
    "                if df[t][manga_ind] == df[t][j]:\n",
    "                    score += genre_score\n",
    "           # if ignore_tfidf_score_above_this_val <tf_idf[j] < ignore_tfidf_score_below_this_val:\n",
    "            score += tf_idf[j] \n",
    "            score += df['rating'][j] - df['rating'][manga_ind]\n",
    "\n",
    "            scores.append([score, df['title'][j]])\n",
    "    scores.sort()\n",
    "    scores.reverse()\n",
    "    return scores[0:10]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying the model on One Piece\n",
    "similarity_score('one piece')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "015882ace943a6af892b59db330d36389659df9a356b8e1b951bdcdb52b46345"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
